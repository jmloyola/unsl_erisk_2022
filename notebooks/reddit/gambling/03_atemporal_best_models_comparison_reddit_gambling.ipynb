{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"../../..\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from src.config import PATH_BEST_MODELS, PATH_REPORTS  # noqa: E402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_KIND = \"reddit\"\n",
    "CORPUS_NAME = \"gambling\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_information(file_path):\n",
    "    with open(file_path) as f:\n",
    "        model_information = json.load(fp=f)\n",
    "    return model_information\n",
    "\n",
    "\n",
    "def get_model_report(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        (\n",
    "            classifier,\n",
    "            classification_report,\n",
    "            precision,\n",
    "            recall,\n",
    "            f1,\n",
    "            accuracy,\n",
    "            confusion_matrix,\n",
    "            elapsed_mins,\n",
    "            elapsed_secs,\n",
    "        ) = pickle.load(f)\n",
    "    output_dictionary = {\n",
    "        \"classification_report\": classification_report,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"confusion_matrix\": confusion_matrix,\n",
    "        \"elapsed_mins\": elapsed_mins,\n",
    "        \"elapsed_secs\": elapsed_secs,\n",
    "    }\n",
    "    return output_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representations_list = [\n",
    "    \"bow\",\n",
    "    \"lda\",\n",
    "    \"lsa\",\n",
    "    \"doc2vec\",\n",
    "    \"padded_sequential\",\n",
    "    \"bert_tokenizer\",\n",
    "]\n",
    "best_measures = [\"f1\", \"positive_f1\"]\n",
    "\n",
    "model_identifier = 0\n",
    "\n",
    "# Dictionary used to create an empty DataFrame with the correct columns.\n",
    "base_dictionary = {\n",
    "    \"model_identifier\": [],\n",
    "    \"corpus_kind\": [],\n",
    "    \"corpus_name\": [],\n",
    "    \"representation\": [],\n",
    "    \"representation_information\": [],\n",
    "    \"train_file_path\": [],\n",
    "    \"random_seed\": [],\n",
    "    \"classifier_type\": [],\n",
    "    \"classifier_params\": [],\n",
    "    \"classification_report\": [],\n",
    "    \"precision\": [],\n",
    "    \"recall\": [],\n",
    "    \"f1\": [],\n",
    "    \"accuracy\": [],\n",
    "    \"confusion_matrix\": [],\n",
    "    \"elapsed_mins\": [],\n",
    "    \"elapsed_secs\": [],\n",
    "    \"total_secs\": [],\n",
    "    \"file_name\": [],\n",
    "}\n",
    "result_comparison = pd.DataFrame(base_dictionary)\n",
    "\n",
    "for current_measure in best_measures:\n",
    "    for representation in representations_list:\n",
    "        base_path = os.path.join(\n",
    "            PATH_BEST_MODELS, current_measure, CORPUS_KIND, CORPUS_NAME, representation\n",
    "        )\n",
    "        possible_json_files = glob.glob(f\"{base_path}/*_model_information.json\")\n",
    "        for json_file in possible_json_files:\n",
    "            pkl_file = json_file[:-16] + \"and_report.pkl\"\n",
    "\n",
    "            model_information = get_model_information(json_file)\n",
    "            output_dictionary = get_model_report(pkl_file)\n",
    "            file_name = os.path.splitext(os.path.basename(json_file))[0]\n",
    "\n",
    "            total_secs = (\n",
    "                output_dictionary[\"elapsed_mins\"] * 60\n",
    "                + output_dictionary[\"elapsed_secs\"]\n",
    "            )\n",
    "\n",
    "            df_row = {\n",
    "                \"model_identifier\": model_identifier,\n",
    "                \"current_measure\": current_measure,\n",
    "                \"file_name\": file_name,\n",
    "                \"total_secs\": total_secs,\n",
    "            }\n",
    "            df_row.update(model_information)\n",
    "            df_row.update(output_dictionary)\n",
    "\n",
    "            result_comparison = result_comparison.append(df_row, ignore_index=True)\n",
    "\n",
    "            model_identifier += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result_comparison.groupby(by=\"representation\")[\"f1\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positive_recall(confusion_matrix):\n",
    "    tp = confusion_matrix.item((1, 1))\n",
    "    fn = confusion_matrix.item((1, 0))\n",
    "    div = tp + fn\n",
    "    if div == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return round(tp / div, 2)\n",
    "\n",
    "\n",
    "def get_positive_precision(confusion_matrix):\n",
    "    tp = confusion_matrix.item((1, 1))\n",
    "    fp = confusion_matrix.item((0, 1))\n",
    "    div = tp + fp\n",
    "    if div == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return round(tp / div, 2)\n",
    "\n",
    "\n",
    "def get_positive_f1_score(confusion_matrix):\n",
    "    tp = confusion_matrix.item((1, 1))\n",
    "    fp = confusion_matrix.item((0, 1))\n",
    "    fn = confusion_matrix.item((1, 0))\n",
    "    div = tp + 0.5 * (fp + fn)\n",
    "    if div == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return round(tp / div, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_comparison[\"positive_recall\"] = result_comparison.confusion_matrix.apply(\n",
    "    get_positive_recall\n",
    ")\n",
    "result_comparison[\"positive_precision\"] = result_comparison.confusion_matrix.apply(\n",
    "    get_positive_precision\n",
    ")\n",
    "result_comparison[\"positive_f1\"] = result_comparison.confusion_matrix.apply(\n",
    "    get_positive_f1_score\n",
    ")\n",
    "result_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_time_training = result_comparison.total_secs.sum() / 60 / 60\n",
    "print(f\"Time spent training all the models: {elapsed_time_training:.02f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_measures = [\"f1\", \"positive_f1\"]\n",
    "\n",
    "for current_measure in best_measures:\n",
    "    today = datetime.today().strftime(\"%Y_%m_%d\")\n",
    "    report_file_name = \"result_comparison_\" + today + \".pkl\"\n",
    "    base_path = os.path.join(\n",
    "        PATH_BEST_MODELS, current_measure, CORPUS_KIND, CORPUS_NAME\n",
    "    )\n",
    "    report_file_path = os.path.join(base_path, report_file_name)\n",
    "\n",
    "    result_comparison[result_comparison.current_measure == current_measure].to_pickle(\n",
    "        path=report_file_path, protocol=4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_time_elapsed(comparison_type=\"representation\", current_measure=\"f1\"):\n",
    "    measure = \"total_secs\"\n",
    "    ncols = 2\n",
    "    figsize = (15, 7)\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=ncols, sharey=False, figsize=figsize)\n",
    "    axes = ax.ravel()\n",
    "\n",
    "    current_df = result_comparison[result_comparison.current_measure == current_measure]\n",
    "\n",
    "    groupby_result = list(current_df.groupby(by=comparison_type)[measure])\n",
    "    data = []\n",
    "    max_values = []\n",
    "    labels = []\n",
    "\n",
    "    # We separated these type of models because they had different values\n",
    "    separated_groups = None\n",
    "    if comparison_type == \"representation\":\n",
    "        separated_groups = [\"padded_sequential\"]\n",
    "    elif comparison_type == \"classifier_type\":\n",
    "        separated_groups = [\"EmbeddingLSTM\"]\n",
    "\n",
    "    for grouped_series in groupby_result:\n",
    "        label = grouped_series[0]\n",
    "        if label in separated_groups:\n",
    "            continue\n",
    "        np_series = grouped_series[1].to_numpy()\n",
    "        labels.append(label)\n",
    "        data.append(np_series)\n",
    "\n",
    "    whis = [2, 98]\n",
    "    axes[0].boxplot(data, whis=whis, labels=labels)\n",
    "    for label in axes[0].get_xticklabels():\n",
    "        label.set_rotation(45)\n",
    "        label.set_ha(\"right\")\n",
    "\n",
    "    max_values = [serie.max() for serie in data]\n",
    "    max_value = max(max_values)\n",
    "    xmin, xmax = 0, 1\n",
    "    axes[0].axhline(\n",
    "        y=max_value,\n",
    "        color=\"r\",\n",
    "        linestyle=\"--\",\n",
    "        xmin=xmin,\n",
    "        xmax=xmax,\n",
    "        label=\"maximum value\",\n",
    "    )\n",
    "    axes[0].legend()\n",
    "\n",
    "    data = []\n",
    "    max_values = []\n",
    "    labels = []\n",
    "    for grouped_series in groupby_result:\n",
    "        label = grouped_series[0]\n",
    "        if label in separated_groups:\n",
    "            np_series = grouped_series[1].to_numpy()\n",
    "            labels.append(label)\n",
    "            data.append(np_series)\n",
    "\n",
    "    axes[1].boxplot(data, whis=whis, labels=labels)\n",
    "    for label in axes[1].get_xticklabels():\n",
    "        label.set_rotation(45)\n",
    "        label.set_ha(\"right\")\n",
    "\n",
    "    max_values = [serie.max() for serie in data]\n",
    "    max_value = max(max_values)\n",
    "    axes[1].axhline(\n",
    "        y=max_value,\n",
    "        color=\"r\",\n",
    "        linestyle=\"--\",\n",
    "        xmin=xmin,\n",
    "        xmax=xmax,\n",
    "        label=\"maximum value\",\n",
    "    )\n",
    "    axes[1].legend()\n",
    "\n",
    "    fig.suptitle(\n",
    "        f'{comparison_type.title()} comparison based on the measure \"{measure}\"'\n",
    "    )\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_boxplot(comparison_type=\"representation\", measure=\"f1\", current_measure=\"f1\"):\n",
    "    double_graph = False\n",
    "    if measure in [\"precision\", \"recall\", \"f1\"]:\n",
    "        double_graph = True\n",
    "\n",
    "    ncols = 2 if double_graph else 1\n",
    "    figsize = (15, 7) if double_graph else (7, 7)\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=ncols, sharey=True, figsize=figsize)\n",
    "    axes = ax.ravel() if double_graph else [ax]\n",
    "\n",
    "    current_df = result_comparison[result_comparison.current_measure == current_measure]\n",
    "\n",
    "    groupby_result = list(current_df.groupby(by=comparison_type)[measure])\n",
    "    data = []\n",
    "    max_values = []\n",
    "    labels = []\n",
    "    for grouped_serie in groupby_result:\n",
    "        label = grouped_serie[0]\n",
    "        np_serie = grouped_serie[1].to_numpy()\n",
    "        labels.append(label)\n",
    "        data.append(np_serie)\n",
    "\n",
    "    axes[0].set_title(\n",
    "        f'{comparison_type.title()} comparison based on the measure \"{measure}\"'\n",
    "    )\n",
    "    whis = [2, 98]\n",
    "    axes[0].boxplot(data, whis=whis, labels=labels)\n",
    "    for label in axes[0].get_xticklabels():\n",
    "        label.set_rotation(45)\n",
    "        label.set_ha(\"right\")\n",
    "\n",
    "    max_values = [serie.max() for serie in data]\n",
    "    max_value = max(max_values)\n",
    "    xmin, xmax = 0, 1\n",
    "    axes[0].axhline(\n",
    "        y=max_value,\n",
    "        color=\"r\",\n",
    "        linestyle=\"--\",\n",
    "        xmin=xmin,\n",
    "        xmax=xmax,\n",
    "        label=\"maximum value\",\n",
    "    )\n",
    "    axes[0].legend()\n",
    "\n",
    "    if double_graph:\n",
    "        second_measure = \"positive_\" + measure\n",
    "        groupby_result = list(current_df.groupby(by=comparison_type)[second_measure])\n",
    "        data = []\n",
    "        max_values = []\n",
    "        labels = []\n",
    "        for grouped_serie in groupby_result:\n",
    "            label = grouped_serie[0]\n",
    "            np_serie = grouped_serie[1].to_numpy()\n",
    "            labels.append(label)\n",
    "            data.append(np_serie)\n",
    "\n",
    "        axes[1].set_title(\n",
    "            f'{comparison_type.title()} comparison based on the measure \"{second_measure}\"'\n",
    "        )\n",
    "        axes[1].boxplot(data, whis=whis, labels=labels)\n",
    "        for label in axes[1].get_xticklabels():\n",
    "            label.set_rotation(45)\n",
    "            label.set_ha(\"right\")\n",
    "\n",
    "        max_values = [serie.max() for serie in data]\n",
    "        max_value = max(max_values)\n",
    "        axes[1].axhline(\n",
    "            y=max_value,\n",
    "            color=\"r\",\n",
    "            linestyle=\"--\",\n",
    "            xmin=xmin,\n",
    "            xmax=xmax,\n",
    "            label=\"maximum value\",\n",
    "        )\n",
    "        axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_matrix_max_values(measure=\"f1\", current_measure=\"f1\"):\n",
    "    double_graph = False\n",
    "    if measure in [\"precision\", \"recall\", \"f1\"]:\n",
    "        double_graph = True\n",
    "\n",
    "    ncols = 2 if double_graph else 1\n",
    "    figsize = (15, 7) if double_graph else (7, 7)\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=ncols, figsize=figsize)\n",
    "    axes = ax.ravel() if double_graph else [ax]\n",
    "\n",
    "    current_df = result_comparison[result_comparison.current_measure == current_measure]\n",
    "\n",
    "    groupby_result = list(\n",
    "        current_df.groupby(by=[\"representation\", \"classifier_type\"])[measure]\n",
    "    )\n",
    "    list_representation = []\n",
    "    list_classifier_type = []\n",
    "    list_max_value = []\n",
    "    for grouped_serie in groupby_result:\n",
    "        representation = grouped_serie[0][0]\n",
    "        classifier_type = grouped_serie[0][1]\n",
    "        max_value = grouped_serie[1].to_numpy().max()\n",
    "\n",
    "        list_representation.append(representation)\n",
    "        list_classifier_type.append(classifier_type)\n",
    "        list_max_value.append(max_value)\n",
    "\n",
    "    data = {\n",
    "        \"representation\": list_representation,\n",
    "        \"classifier_type\": list_classifier_type,\n",
    "        \"max_value\": list_max_value,\n",
    "    }\n",
    "    df = pd.DataFrame(data=data)\n",
    "    df = df.pivot(\"representation\", \"classifier_type\", \"max_value\")\n",
    "    sns.heatmap(df, ax=axes[0], annot=True, fmt=\".2f\", vmin=0, vmax=1, cmap=\"RdYlGn\")\n",
    "\n",
    "    axes[0].set_title(f'Maximum values for the measure \"{measure}\"')\n",
    "\n",
    "    if double_graph:\n",
    "        second_measure = \"positive_\" + measure\n",
    "        groupby_result = list(\n",
    "            current_df.groupby(by=[\"representation\", \"classifier_type\"])[measure]\n",
    "        )\n",
    "        list_representation = []\n",
    "        list_classifier_type = []\n",
    "        list_max_value = []\n",
    "        for grouped_serie in groupby_result:\n",
    "            representation = grouped_serie[0][0]\n",
    "            classifier_type = grouped_serie[0][1]\n",
    "            max_value = grouped_serie[1].to_numpy().max()\n",
    "\n",
    "            list_representation.append(representation)\n",
    "            list_classifier_type.append(classifier_type)\n",
    "            list_max_value.append(max_value)\n",
    "\n",
    "        data = {\n",
    "            \"representation\": list_representation,\n",
    "            \"classifier_type\": list_classifier_type,\n",
    "            \"max_value\": list_max_value,\n",
    "        }\n",
    "        df = pd.DataFrame(data=data)\n",
    "        df = df.pivot(\"representation\", \"classifier_type\", \"max_value\")\n",
    "        sns.heatmap(\n",
    "            df, ax=axes[1], annot=True, fmt=\".2f\", vmin=0, vmax=1, cmap=\"RdYlGn\"\n",
    "        )\n",
    "\n",
    "        axes[1].set_title(f'Maximum values for the measure \"{second_measure}\"')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_REPORT_FIGURES = os.path.join(\n",
    "    PATH_REPORTS, CORPUS_KIND, CORPUS_NAME, \"01_atemporal_classification_erisk/figures\"\n",
    ")\n",
    "os.makedirs(PATH_TO_REPORT_FIGURES, exist_ok=True)\n",
    "fig_dip = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for the models with better values for the measure `f1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = show_time_elapsed(comparison_type=\"representation\", current_measure=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = show_time_elapsed(comparison_type=\"classifier_type\", current_measure=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = show_boxplot(\n",
    "    comparison_type=\"representation\", measure=\"accuracy\", current_measure=\"f1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = show_boxplot(\n",
    "    comparison_type=\"representation\", measure=\"precision\", current_measure=\"f1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = show_boxplot(\n",
    "    comparison_type=\"representation\", measure=\"recall\", current_measure=\"f1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = show_boxplot(comparison_type=\"representation\", measure=\"f1\", current_measure=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = show_boxplot(\n",
    "    comparison_type=\"classifier_type\", measure=\"accuracy\", current_measure=\"f1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = show_boxplot(\n",
    "    comparison_type=\"classifier_type\", measure=\"precision\", current_measure=\"f1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = show_boxplot(\n",
    "    comparison_type=\"classifier_type\", measure=\"recall\", current_measure=\"f1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = show_boxplot(comparison_type=\"classifier_type\", measure=\"f1\", current_measure=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = show_matrix_max_values(measure=\"accuracy\", current_measure=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = show_matrix_max_values(measure=\"precision\", current_measure=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = show_matrix_max_values(measure=\"recall\", current_measure=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "figure = show_matrix_max_values(measure=\"f1\", current_measure=\"f1\")\n",
    "\n",
    "file_name = \"10_f1_best_experiments_f1_max_values.png\"\n",
    "file_path = os.path.join(PATH_TO_REPORT_FIGURES, file_name)\n",
    "figure.savefig(fname=file_path, dpi=fig_dip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for the models with better values for the measure `positive_f1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = show_time_elapsed(comparison_type=\"representation\", current_measure=\"positive_f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = show_time_elapsed(comparison_type=\"classifier_type\", current_measure=\"positive_f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = show_boxplot(\n",
    "    comparison_type=\"representation\", measure=\"accuracy\", current_measure=\"positive_f1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = show_boxplot(\n",
    "    comparison_type=\"representation\", measure=\"precision\", current_measure=\"positive_f1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = show_boxplot(\n",
    "    comparison_type=\"representation\", measure=\"recall\", current_measure=\"positive_f1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = show_boxplot(\n",
    "    comparison_type=\"representation\", measure=\"f1\", current_measure=\"positive_f1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = show_boxplot(\n",
    "    comparison_type=\"classifier_type\", measure=\"accuracy\", current_measure=\"positive_f1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = show_boxplot(\n",
    "    comparison_type=\"classifier_type\",\n",
    "    measure=\"precision\",\n",
    "    current_measure=\"positive_f1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = show_boxplot(\n",
    "    comparison_type=\"classifier_type\", measure=\"recall\", current_measure=\"positive_f1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = show_boxplot(\n",
    "    comparison_type=\"classifier_type\", measure=\"f1\", current_measure=\"positive_f1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = show_matrix_max_values(measure=\"accuracy\", current_measure=\"positive_f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = show_matrix_max_values(measure=\"precision\", current_measure=\"positive_f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = show_matrix_max_values(measure=\"recall\", current_measure=\"positive_f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = show_matrix_max_values(measure=\"f1\", current_measure=\"positive_f1\")\n",
    "\n",
    "file_name = \"11_positive_f1_best_experiments_f1_max_values.png\"\n",
    "file_path = os.path.join(PATH_TO_REPORT_FIGURES, file_name)\n",
    "figure.savefig(fname=file_path, dpi=fig_dip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze the experiment replicas\n",
    "We observe the distribution of times and the different performance measurements for the same parameters, only alternating the random seed or shuffling the corpus to obtain other results (in some models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_replica_results_subplot(\n",
    "    representation=\"bow\", measure=\"f1\", current_measure=\"f1\", ax=None\n",
    "):\n",
    "    if ax is None:\n",
    "        figsize = (7, 7)\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=1, figsize=figsize)\n",
    "\n",
    "    current_df = result_comparison.loc[\n",
    "        (result_comparison.representation == representation)\n",
    "        & (result_comparison.current_measure == current_measure)\n",
    "    ]\n",
    "\n",
    "    groupby_result = list(current_df.groupby(by=\"classifier_type\")[measure])\n",
    "    data = []\n",
    "    max_values = []\n",
    "    labels = []\n",
    "    for grouped_serie in groupby_result:\n",
    "        label = grouped_serie[0]\n",
    "        np_serie = grouped_serie[1].to_numpy()\n",
    "        labels.append(label)\n",
    "        data.append(np_serie)\n",
    "\n",
    "    ax.set_title(\n",
    "        f'Experiments replicas that obatained the best \"{current_measure}\" for\\n'\n",
    "        f'{representation} looking at \"{measure}\"'\n",
    "    )\n",
    "    whis = [2, 98]\n",
    "    ax.boxplot(data, whis=whis, labels=labels)\n",
    "    for label in ax.get_xticklabels():\n",
    "        label.set_rotation(45)\n",
    "        label.set_ha(\"right\")\n",
    "\n",
    "    max_values = [serie.max() for serie in data]\n",
    "    max_value = max(max_values)\n",
    "    xmin, xmax = 0, 1\n",
    "    ax.axhline(\n",
    "        y=max_value,\n",
    "        color=\"r\",\n",
    "        linestyle=\"--\",\n",
    "        xmin=xmin,\n",
    "        xmax=xmax,\n",
    "        label=\"maximum value\",\n",
    "    )\n",
    "    ax.legend()\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_replica_results(measure=\"f1\", current_measure=\"f1\"):\n",
    "    representations_list = result_comparison.representation.unique().tolist()\n",
    "\n",
    "    nrows = (len(representations_list) + 1) // 2\n",
    "    figsize = (15, 15)\n",
    "    fig, ax = plt.subplots(nrows=nrows, ncols=2, sharey=False, figsize=figsize)\n",
    "    axes = ax.ravel()\n",
    "\n",
    "    for i, representation in enumerate(representations_list):\n",
    "        get_replica_results_subplot(\n",
    "            representation=representation,\n",
    "            measure=measure,\n",
    "            current_measure=current_measure,\n",
    "            ax=axes[i],\n",
    "        )\n",
    "    if len(representations_list) % 2 == 1:\n",
    "        axes[-1].set_axis_off()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for the replicas with better values for the measure `f1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_ = show_replica_results(measure=\"total_secs\", current_measure=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = show_replica_results(measure=\"f1\", current_measure=\"f1\")\n",
    "\n",
    "file_name = \"12_f1_best_experiments_f1_comparison.png\"\n",
    "file_path = os.path.join(PATH_TO_REPORT_FIGURES, file_name)\n",
    "figure.savefig(fname=file_path, dpi=fig_dip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_ = show_replica_results(measure=\"positive_f1\", current_measure=\"f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for the replicas with better values for the measure `positive_f1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_ = show_replica_results(measure=\"total_secs\", current_measure=\"positive_f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_ = show_replica_results(measure=\"f1\", current_measure=\"positive_f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = show_replica_results(measure=\"positive_f1\", current_measure=\"positive_f1\")\n",
    "\n",
    "file_name = \"13_positive_f1_best_experiments_positive_f1_comparison.png\"\n",
    "file_path = os.path.join(PATH_TO_REPORT_FIGURES, file_name)\n",
    "figure.savefig(fname=file_path, dpi=fig_dip)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
