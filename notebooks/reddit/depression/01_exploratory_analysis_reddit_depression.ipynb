{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"../../..\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from src.config import END_OF_POST_TOKEN, PATH_INTERIM_CORPUS  # noqa: E402\n",
    "from src.features.build_features import get_corpus_id2word  # noqa: E402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_KIND = \"reddit\"\n",
    "CORPUS_NAME = \"depression\"\n",
    "NUM_SUB_PROCESSES = int(2 * os.cpu_count() / 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a DataFrame for the training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path_train = os.path.join(\n",
    "    PATH_INTERIM_CORPUS, CORPUS_KIND, CORPUS_NAME, f\"{CORPUS_NAME}-train-clean.txt\"\n",
    ")\n",
    "input_file_path_test = os.path.join(\n",
    "    PATH_INTERIM_CORPUS, CORPUS_KIND, CORPUS_NAME, f\"{CORPUS_NAME}-test-clean.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "documents = []\n",
    "with open(input_file_path_train) as f:\n",
    "    for line in f:\n",
    "        label, document = line.split(maxsplit=1)\n",
    "        labels.append(label)\n",
    "        posts = document.split(END_OF_POST_TOKEN)\n",
    "        documents.append(posts)\n",
    "df_train = pd.DataFrame({\"label\": labels, \"posts\": documents})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "documents = []\n",
    "with open(input_file_path_test) as f:\n",
    "    for line in f:\n",
    "        label, document = line.split(maxsplit=1)\n",
    "        labels.append(label)\n",
    "        posts = document.split(END_OF_POST_TOKEN)\n",
    "        documents.append(posts)\n",
    "df_test = pd.DataFrame({\"label\": labels, \"posts\": documents})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions to obtain interesting features from the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextBlob(df_test.posts[0][0]).sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_tokens(posts_list):\n",
    "    return [len(post.split()) for post in posts_list]\n",
    "\n",
    "\n",
    "def get_num_tokens_first_person(posts_list):\n",
    "    return [\n",
    "        sum(1 if word == \"i\" else 0 for word in post.split()) for post in posts_list\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_polarity(posts_list):\n",
    "    return [round(TextBlob(post).sentiment.polarity, 2) for post in posts_list]\n",
    "\n",
    "\n",
    "def get_subjectivity(posts_list):\n",
    "    return [round(TextBlob(post).sentiment.subjectivity, 2) for post in posts_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply functions to the training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"num_tokens\"] = df_train.posts.apply(get_num_tokens)\n",
    "df_train[\"num_tokens_first_person\"] = df_train.posts.apply(get_num_tokens_first_person)\n",
    "df_train[\"polarity\"] = df_train.posts.apply(get_polarity)\n",
    "df_train[\"subjectivity\"] = df_train.posts.apply(get_subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply functions to the testing corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"num_tokens\"] = df_test.posts.apply(get_num_tokens)\n",
    "df_test[\"num_tokens_first_person\"] = df_test.posts.apply(get_num_tokens_first_person)\n",
    "df_test[\"polarity\"] = df_test.posts.apply(get_polarity)\n",
    "df_test[\"subjectivity\"] = df_test.posts.apply(get_subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(8, 3))\n",
    "ax1.set_title(\"Training\")\n",
    "ax2.set_title(\"Testing\")\n",
    "fig.suptitle(\"Number of users with depression\")\n",
    "sns.countplot(x=\"label\", data=df_train, ax=ax1)\n",
    "sns.countplot(x=\"label\", data=df_test, ax=ax2)\n",
    "plt.tight_layout(pad=2.8, w_pad=0.5, h_pad=1.0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens_train = df_train.groupby(\"label\").agg({\"num_tokens\": sum})\n",
    "num_tokens_train = num_tokens_train.num_tokens.apply(lambda x: np.array(x))\n",
    "num_tokens_train.apply(lambda x: x.sort())\n",
    "\n",
    "num_tokens_test = df_test.groupby(\"label\").agg({\"num_tokens\": sum})\n",
    "num_tokens_test = num_tokens_test.num_tokens.apply(lambda x: np.array(x))\n",
    "num_tokens_test.apply(lambda x: x.sort())\n",
    "\n",
    "fig, axis = plt.subplots(nrows=2, ncols=2, sharey=False, figsize=(10, 6))\n",
    "\n",
    "axis[0, 0].set_title(\"Training\")\n",
    "axis[0, 1].set_title(\"Testing\")\n",
    "fig.suptitle(\"Posts length\")\n",
    "\n",
    "sns.kdeplot(num_tokens_train[\"positive\"], ax=axis[0, 0], label=\"positive\")\n",
    "sns.kdeplot(num_tokens_train[\"negative\"], ax=axis[0, 0], label=\"negative\")\n",
    "\n",
    "sns.kdeplot(num_tokens_test[\"positive\"], ax=axis[0, 1], label=\"positive\")\n",
    "sns.kdeplot(num_tokens_test[\"negative\"], ax=axis[0, 1], label=\"negative\")\n",
    "\n",
    "sns.distplot(num_tokens_train[\"positive\"], ax=axis[1, 0], label=\"positive\", kde=False)\n",
    "sns.distplot(num_tokens_train[\"negative\"], ax=axis[1, 0], label=\"negative\", kde=False)\n",
    "\n",
    "sns.distplot(num_tokens_test[\"positive\"], ax=axis[1, 1], label=\"positive\", kde=False)\n",
    "sns.distplot(num_tokens_test[\"negative\"], ax=axis[1, 1], label=\"negative\", kde=False)\n",
    "\n",
    "for ax in axis.ravel():\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout(pad=2.8, w_pad=0.5, h_pad=1.0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_train_negative, outlier_train_positive = (\n",
    "    np.quantile(num_tokens_train[0], 0.97),\n",
    "    np.quantile(num_tokens_train[1], 0.97),\n",
    ")\n",
    "outlier_test_negative, outlier_test_positive = (\n",
    "    np.quantile(num_tokens_test[0], 0.97),\n",
    "    np.quantile(num_tokens_test[1], 0.97),\n",
    ")\n",
    "\n",
    "fig, axis = plt.subplots(nrows=2, ncols=2, sharey=False, figsize=(10, 6))\n",
    "axis[0, 0].set_title(\"Training\")\n",
    "axis[0, 1].set_title(\"Testing\")\n",
    "fig.suptitle(\"Posts length (without outliers)\")\n",
    "\n",
    "without_outliers = num_tokens_train[\"positive\"][\n",
    "    num_tokens_train[\"positive\"] < outlier_train_positive\n",
    "]\n",
    "sns.kdeplot(without_outliers, ax=axis[0, 0], label=\"positive\")\n",
    "sns.distplot(without_outliers, ax=axis[1, 0], label=\"positive\", kde=False)\n",
    "\n",
    "without_outliers = num_tokens_train[\"negative\"][\n",
    "    num_tokens_train[\"negative\"] < outlier_train_negative\n",
    "]\n",
    "sns.kdeplot(without_outliers, ax=axis[0, 0], label=\"negative\")\n",
    "sns.distplot(without_outliers, ax=axis[1, 0], label=\"negative\", kde=False)\n",
    "\n",
    "without_outliers = num_tokens_test[\"positive\"][\n",
    "    num_tokens_test[\"positive\"] < outlier_test_positive\n",
    "]\n",
    "sns.kdeplot(without_outliers, ax=axis[0, 1], label=\"positive\")\n",
    "sns.distplot(without_outliers, ax=axis[1, 1], label=\"positive\", kde=False)\n",
    "\n",
    "without_outliers = num_tokens_test[\"negative\"][\n",
    "    num_tokens_test[\"negative\"] < outlier_test_negative\n",
    "]\n",
    "sns.kdeplot(without_outliers, ax=axis[0, 1], label=\"negative\")\n",
    "sns.distplot(without_outliers, ax=axis[1, 1], label=\"negative\", kde=False)\n",
    "\n",
    "for ax in axis.ravel():\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout(pad=2.8, w_pad=0.5, h_pad=1.0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_first_person_train = df_train.groupby(\"label\").agg({\"num_tokens_first_person\": sum})\n",
    "num_first_person_train = num_first_person_train.num_tokens_first_person.apply(\n",
    "    lambda x: np.array(x)\n",
    ")\n",
    "num_first_person_train.apply(lambda x: x.sort())\n",
    "\n",
    "num_first_person_test = df_test.groupby(\"label\").agg({\"num_tokens_first_person\": sum})\n",
    "num_first_person_test = num_first_person_test.num_tokens_first_person.apply(\n",
    "    lambda x: np.array(x)\n",
    ")\n",
    "num_first_person_test.apply(lambda x: x.sort())\n",
    "\n",
    "fig, axis = plt.subplots(nrows=2, ncols=2, sharey=False, figsize=(10, 6))\n",
    "axis[0, 0].set_title(\"Training\")\n",
    "axis[0, 1].set_title(\"Testing\")\n",
    "fig.suptitle(\"References to the first person in the posts\")\n",
    "\n",
    "sns.kdeplot(num_first_person_train[\"positive\"], ax=axis[0, 0], label=\"positive\")\n",
    "sns.distplot(\n",
    "    num_first_person_train[\"positive\"], ax=axis[1, 0], label=\"positive\", kde=False\n",
    ")\n",
    "sns.kdeplot(num_first_person_train[\"negative\"], ax=axis[0, 0], label=\"negative\")\n",
    "sns.distplot(\n",
    "    num_first_person_train[\"negative\"], ax=axis[1, 0], label=\"negative\", kde=False\n",
    ")\n",
    "\n",
    "sns.kdeplot(num_first_person_test[\"positive\"], ax=axis[0, 1], label=\"positive\")\n",
    "sns.distplot(\n",
    "    num_first_person_test[\"positive\"], ax=axis[1, 1], label=\"positive\", kde=False\n",
    ")\n",
    "sns.kdeplot(num_first_person_test[\"negative\"], ax=axis[0, 1], label=\"negative\")\n",
    "sns.distplot(\n",
    "    num_first_person_test[\"negative\"], ax=axis[1, 1], label=\"negative\", kde=False\n",
    ")\n",
    "\n",
    "for ax in axis.ravel():\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout(pad=2.8, w_pad=0.5, h_pad=1.0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_threshold = 0.98\n",
    "outlier_train_negative, outlier_train_positive = (\n",
    "    np.quantile(num_first_person_train[0], outlier_threshold),\n",
    "    np.quantile(num_first_person_train[1], outlier_threshold),\n",
    ")\n",
    "outlier_test_negative, outlier_test_positive = (\n",
    "    np.quantile(num_first_person_test[0], outlier_threshold),\n",
    "    np.quantile(num_first_person_test[1], outlier_threshold),\n",
    ")\n",
    "\n",
    "fig, axis = plt.subplots(nrows=2, ncols=2, sharey=False, figsize=(10, 6))\n",
    "axis[0, 0].set_title(\"Training\")\n",
    "axis[0, 1].set_title(\"Testing\")\n",
    "fig.suptitle(\"References to the first person in the posts (without outliers)\")\n",
    "\n",
    "without_outliers = num_first_person_train[\"positive\"][\n",
    "    num_first_person_train[\"positive\"] < outlier_train_positive\n",
    "]\n",
    "sns.kdeplot(without_outliers, ax=axis[0, 0], label=\"positive\")\n",
    "sns.distplot(without_outliers, ax=axis[1, 0], label=\"positive\", kde=False)\n",
    "\n",
    "without_outliers = num_first_person_train[\"negative\"][\n",
    "    num_first_person_train[\"negative\"] < outlier_train_negative\n",
    "]\n",
    "sns.kdeplot(without_outliers, ax=axis[0, 0], label=\"negative\")\n",
    "sns.distplot(without_outliers, ax=axis[1, 0], label=\"negative\", kde=False)\n",
    "\n",
    "without_outliers = num_first_person_test[\"positive\"][\n",
    "    num_first_person_test[\"positive\"] < outlier_test_positive\n",
    "]\n",
    "sns.kdeplot(without_outliers, ax=axis[0, 1], label=\"positive\")\n",
    "sns.distplot(without_outliers, ax=axis[1, 1], label=\"positive\", kde=False)\n",
    "\n",
    "without_outliers = num_first_person_test[\"negative\"][\n",
    "    num_first_person_test[\"negative\"] < outlier_test_negative\n",
    "]\n",
    "sns.kdeplot(without_outliers, ax=axis[0, 1], label=\"negative\")\n",
    "sns.distplot(without_outliers, ax=axis[1, 1], label=\"negative\", kde=False)\n",
    "\n",
    "for ax in axis.ravel():\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout(pad=2.8, w_pad=0.5, h_pad=1.0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA)\n",
    "We apply LDA to obtain the most relevant topics from documents of both positive and negative people for depression problems, both for training and for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = []\n",
    "for user_posts in df_train.posts:\n",
    "    aux_list_posts = []\n",
    "    for post in user_posts:\n",
    "        for word in post.split():\n",
    "            aux_list_posts.append(word)\n",
    "    posts.append(aux_list_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, id2word, bigram = get_corpus_id2word(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_train = gensim.models.LdaModel(\n",
    "    corpus=corpus,\n",
    "    num_topics=15,\n",
    "    id2word=id2word,\n",
    "    chunksize=100,\n",
    "    passes=50,\n",
    "    eval_every=1,\n",
    "    random_state=30,\n",
    "    per_word_topics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_train.print_topics(num_topics=-1, num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topics = lda_train.get_document_topics(corpus[1], minimum_probability=0.0)\n",
    "topic_vec = [top_topics[i][1] for i in range(10)]\n",
    "top_topics, topic_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_test = []\n",
    "for user_posts in df_test.posts:\n",
    "    aux_list_posts = []\n",
    "    for post in user_posts:\n",
    "        for word in post.split():\n",
    "            aux_list_posts.append(word)\n",
    "    posts_test.append(aux_list_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_test[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_test, _, _ = get_corpus_id2word(posts_test, bigram_model=bigram, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topics_test = lda_train.get_document_topics(corpus_test[1], minimum_probability=0.0)\n",
    "topic_vec_test = [top_topics_test[i][1] for i in range(10)]\n",
    "top_topics_test, topic_vec_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs related to the posts sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity_train = df_train.groupby(\"label\").agg({\"polarity\": sum})\n",
    "polarity_train = polarity_train.polarity.apply(lambda x: np.array(x))\n",
    "\n",
    "polarity_test = df_test.groupby(\"label\").agg({\"polarity\": sum})\n",
    "polarity_test = polarity_test.polarity.apply(lambda x: np.array(x))\n",
    "\n",
    "fig, axis = plt.subplots(nrows=2, ncols=2, sharey=False, figsize=(10, 6))\n",
    "axis[0, 0].set_title(\"Training\")\n",
    "axis[0, 1].set_title(\"Testing\")\n",
    "fig.suptitle(\"Polarity of the posts\")\n",
    "\n",
    "sns.kdeplot(polarity_train[\"positive\"], ax=axis[0, 0], label=\"positive\")\n",
    "sns.distplot(polarity_train[\"positive\"], ax=axis[1, 0], label=\"positive\", kde=False)\n",
    "sns.kdeplot(polarity_train[\"negative\"], ax=axis[0, 0], label=\"negative\")\n",
    "sns.distplot(polarity_train[\"negative\"], ax=axis[1, 0], label=\"negative\", kde=False)\n",
    "\n",
    "sns.kdeplot(polarity_test[\"positive\"], ax=axis[0, 1], label=\"positive\")\n",
    "sns.distplot(polarity_test[\"positive\"], ax=axis[1, 1], label=\"positive\", kde=False)\n",
    "sns.kdeplot(polarity_test[\"negative\"], ax=axis[0, 1], label=\"negative\")\n",
    "sns.distplot(polarity_test[\"negative\"], ax=axis[1, 1], label=\"negative\", kde=False)\n",
    "\n",
    "for ax in axis.ravel():\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout(pad=2.8, w_pad=0.5, h_pad=1.0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjectivity_train = df_train.groupby(\"label\").agg({\"subjectivity\": sum})\n",
    "subjectivity_train = subjectivity_train.subjectivity.apply(lambda x: np.array(x))\n",
    "\n",
    "subjectivity_test = df_test.groupby(\"label\").agg({\"subjectivity\": sum})\n",
    "subjectivity_test = subjectivity_test.subjectivity.apply(lambda x: np.array(x))\n",
    "\n",
    "fig, axis = plt.subplots(nrows=2, ncols=2, sharey=False, figsize=(10, 6))\n",
    "axis[0, 0].set_title(\"Training\")\n",
    "axis[0, 1].set_title(\"Testing\")\n",
    "fig.suptitle(\"Subjectivity of the posts\")\n",
    "\n",
    "sns.kdeplot(subjectivity_train[\"positive\"], ax=axis[0, 0], label=\"positive\")\n",
    "sns.distplot(subjectivity_train[\"positive\"], ax=axis[1, 0], label=\"positive\", kde=False)\n",
    "sns.kdeplot(subjectivity_train[\"negative\"], ax=axis[0, 0], label=\"negative\")\n",
    "sns.distplot(subjectivity_train[\"negative\"], ax=axis[1, 0], label=\"negative\", kde=False)\n",
    "\n",
    "sns.kdeplot(subjectivity_test[\"positive\"], ax=axis[0, 1], label=\"positive\")\n",
    "sns.distplot(subjectivity_test[\"positive\"], ax=axis[1, 1], label=\"positive\", kde=False)\n",
    "sns.kdeplot(subjectivity_test[\"negative\"], ax=axis[0, 1], label=\"negative\")\n",
    "sns.distplot(subjectivity_test[\"negative\"], ax=axis[1, 1], label=\"negative\", kde=False)\n",
    "\n",
    "for ax in axis.ravel():\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout(pad=2.8, w_pad=0.5, h_pad=1.0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_train_posts = [p for _posts in df_train.posts for p in _posts]\n",
    "plain_test_posts = [p for _posts in df_test.posts for p in _posts]\n",
    "\n",
    "plain_posts = plain_train_posts + plain_test_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b[\\w']+\\b\", use_idf=True)\n",
    "vectorizer.fit(plain_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {idx: word for (word, idx) in vectorizer.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idf_sort_idxs = np.argsort(vectorizer.idf_)\n",
    "\n",
    "print(\"Top 10 words with biggest IDF\")\n",
    "for i in idf_sort_idxs[:10]:\n",
    "    print(f\"'{idx2word[i]}': {vectorizer.idf_[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of idf values throughout the dataset.\n",
    "sns.distplot(vectorizer.idf_, kde=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_percentile = 0.1\n",
    "min_idf_allowed = np.percentile(vectorizer.idf_, min_percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_in_vectorizer(posts_list):\n",
    "    return [\n",
    "        word\n",
    "        for post in posts_list\n",
    "        for word in post.split()\n",
    "        if (word in vectorizer.vocabulary_)\n",
    "        and (vectorizer.idf_[vectorizer.vocabulary_[word]] > min_idf_allowed)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "users_posts = df_train.posts\n",
    "\n",
    "with Pool(processes=NUM_SUB_PROCESSES) as pool:\n",
    "    result_users_posts = pool.map(get_words_in_vectorizer, users_posts)\n",
    "result_users_posts = [\" \".join(r) for r in result_users_posts]\n",
    "result_users_posts = \" \".join(result_users_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=1600, height=800, background_color=\"white\").generate(\n",
    "    result_users_posts\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(30, 10), facecolor=\"white\")\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\n",
    "    \"The top 100 most used words in the training corpus\\n(without considering the \"\n",
    "    f\"{min_percentile}%\\nwords with least value of idf)\",\n",
    "    fontsize=70,\n",
    ")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mostcommon_small_training = FreqDist(result_users_posts.split()).most_common(25)\n",
    "\n",
    "x, y = zip(*mostcommon_small_training)\n",
    "plt.figure(figsize=(50, 30))\n",
    "plt.margins(0.02)\n",
    "plt.bar(x, y)\n",
    "plt.xlabel(\"Words\", fontsize=50)\n",
    "plt.ylabel(\"Frequency of Words\", fontsize=50)\n",
    "plt.yticks(fontsize=40)\n",
    "plt.xticks(rotation=60, fontsize=40)\n",
    "plt.title(\n",
    "    \"Frecuency of the 25 most common words for the training corpus\\n(without considering the \"\n",
    "    f\"{min_percentile}%\\nwords with least value of idf)\",\n",
    "    fontsize=60,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud for positive users in the training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_users_posts = df_train.posts[df_train.label == \"positive\"]\n",
    "\n",
    "with Pool(processes=NUM_SUB_PROCESSES) as pool:\n",
    "    result_positive_users_posts = pool.map(\n",
    "        get_words_in_vectorizer, positive_users_posts\n",
    "    )\n",
    "result_positive_users_posts = [\" \".join(r) for r in result_positive_users_posts]\n",
    "result_positive_users_posts = \" \".join(result_positive_users_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "positive_wordcloud = WordCloud(\n",
    "    width=1600, height=800, background_color=\"white\"\n",
    ").generate(result_positive_users_posts)\n",
    "\n",
    "fig = plt.figure(figsize=(30, 10), facecolor=\"white\")\n",
    "plt.imshow(positive_wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\n",
    "    \"Training corpus WordCloud for the positive users\\n(without considering the \"\n",
    "    f\"{min_percentile}% words with least value of idf)\",\n",
    "    fontsize=70,\n",
    ")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "positive_mostcommon_small = FreqDist(result_positive_users_posts.split()).most_common(\n",
    "    25\n",
    ")\n",
    "\n",
    "x, y = zip(*positive_mostcommon_small)\n",
    "plt.figure(figsize=(50, 30))\n",
    "plt.margins(0.02)\n",
    "plt.bar(x, y)\n",
    "plt.xlabel(\"Words\", fontsize=50)\n",
    "plt.ylabel(\"Frequency of Words\", fontsize=50)\n",
    "plt.yticks(fontsize=40)\n",
    "plt.xticks(rotation=60, fontsize=40)\n",
    "plt.title(\n",
    "    \"Frecuency of the 25 most common words for the training corpus\\nfor the positive users (without considering the \"\n",
    "    f\"{min_percentile}%\\nwords with least value of idf)\",\n",
    "    fontsize=60,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud for negative users in the training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_users_posts = df_train.posts[df_train.label == \"negative\"]\n",
    "\n",
    "with Pool(processes=NUM_SUB_PROCESSES) as pool:\n",
    "    result_negative_users_posts = pool.map(\n",
    "        get_words_in_vectorizer, negative_users_posts\n",
    "    )\n",
    "result_negative_users_posts = [\" \".join(r) for r in result_negative_users_posts]\n",
    "result_negative_users_posts = \" \".join(result_negative_users_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "negative_wordcloud = WordCloud(\n",
    "    width=1600, height=800, background_color=\"white\"\n",
    ").generate(result_negative_users_posts)\n",
    "\n",
    "fig = plt.figure(figsize=(30, 10), facecolor=\"white\")\n",
    "plt.imshow(negative_wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\n",
    "    \"The top 100 most used words in the training corpus\\nfor the negative users (without considering the \"\n",
    "    f\"{min_percentile}%\\nwords with least value of idf)\",\n",
    "    fontsize=70,\n",
    ")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "negative_mostcommon_small = FreqDist(result_negative_users_posts.split()).most_common(\n",
    "    25\n",
    ")\n",
    "\n",
    "x, y = zip(*negative_mostcommon_small)\n",
    "plt.figure(figsize=(50, 30))\n",
    "plt.margins(0.02)\n",
    "plt.bar(x, y)\n",
    "plt.xlabel(\"Words\", fontsize=50)\n",
    "plt.ylabel(\"Frequency of Words\", fontsize=50)\n",
    "plt.yticks(fontsize=40)\n",
    "plt.xticks(rotation=60, fontsize=40)\n",
    "plt.title(\n",
    "    \"Frecuency of the 25 most common words for the training corpus\\nfor the negative users (without considering the \"\n",
    "    f\"{min_percentile}% words with least value of idf)\",\n",
    "    fontsize=60,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_posts = df_test.posts\n",
    "\n",
    "with Pool(processes=NUM_SUB_PROCESSES) as pool:\n",
    "    result_users_posts = pool.map(get_words_in_vectorizer, users_posts)\n",
    "result_users_posts = [\" \".join(r) for r in result_users_posts]\n",
    "result_users_posts = \" \".join(result_users_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=1600, height=800, background_color=\"white\").generate(\n",
    "    result_users_posts\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(30, 10), facecolor=\"white\")\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\n",
    "    \"The top 100 most used words in the testing corpus\\n(without considering the \"\n",
    "    f\"{min_percentile}%\\nwords with least value of idf)\",\n",
    "    fontsize=70,\n",
    ")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mostcommon_small = FreqDist(result_users_posts.split()).most_common(25)\n",
    "\n",
    "x, y = zip(*mostcommon_small)\n",
    "plt.figure(figsize=(50, 30))\n",
    "plt.margins(0.02)\n",
    "plt.bar(x, y)\n",
    "plt.xlabel(\"Words\", fontsize=50)\n",
    "plt.ylabel(\"Frequency of Words\", fontsize=50)\n",
    "plt.yticks(fontsize=40)\n",
    "plt.xticks(rotation=60, fontsize=40)\n",
    "plt.title(\n",
    "    \"Frecuency of the 25 most common words for the testing corpus\\n(without considering the \"\n",
    "    f\"{min_percentile}%\\nwords with least value of idf)\",\n",
    "    fontsize=60,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud for positive users in the testing corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_users_posts = df_test.posts[df_test.label == \"positive\"]\n",
    "\n",
    "with Pool(processes=NUM_SUB_PROCESSES) as pool:\n",
    "    result_positive_users_posts = pool.map(\n",
    "        get_words_in_vectorizer, positive_users_posts\n",
    "    )\n",
    "result_positive_users_posts = [\" \".join(r) for r in result_positive_users_posts]\n",
    "result_positive_users_posts = \" \".join(result_positive_users_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "positive_wordcloud = WordCloud(\n",
    "    width=1600, height=800, background_color=\"white\"\n",
    ").generate(result_positive_users_posts)\n",
    "\n",
    "fig = plt.figure(figsize=(30, 10), facecolor=\"white\")\n",
    "plt.imshow(positive_wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\n",
    "    \"Testing corpus WordCloud for the positive users\\n(without considering the \"\n",
    "    f\"{min_percentile}% words with least value of idf)\",\n",
    "    fontsize=70,\n",
    ")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "positive_mostcommon_small = FreqDist(result_positive_users_posts.split()).most_common(\n",
    "    25\n",
    ")\n",
    "\n",
    "x, y = zip(*positive_mostcommon_small)\n",
    "plt.figure(figsize=(50, 30))\n",
    "plt.margins(0.02)\n",
    "plt.bar(x, y)\n",
    "plt.xlabel(\"Words\", fontsize=50)\n",
    "plt.ylabel(\"Frequency of Words\", fontsize=50)\n",
    "plt.yticks(fontsize=40)\n",
    "plt.xticks(rotation=60, fontsize=40)\n",
    "plt.title(\n",
    "    \"Frecuency of the 25 most common words for the testing corpus\\nfor the positive users (without considering the \"\n",
    "    f\"{min_percentile}%\\nwords with least value of idf)\",\n",
    "    fontsize=60,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud for negative users in the testing corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_users_posts = df_test.posts[df_test.label == \"negative\"]\n",
    "\n",
    "with Pool(processes=NUM_SUB_PROCESSES) as pool:\n",
    "    result_negative_users_posts = pool.map(\n",
    "        get_words_in_vectorizer, negative_users_posts\n",
    "    )\n",
    "result_negative_users_posts = [\" \".join(r) for r in result_negative_users_posts]\n",
    "result_negative_users_posts = \" \".join(result_negative_users_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "negative_wordcloud = WordCloud(\n",
    "    width=1600, height=800, background_color=\"white\"\n",
    ").generate(result_negative_users_posts)\n",
    "\n",
    "fig = plt.figure(figsize=(30, 10), facecolor=\"white\")\n",
    "plt.imshow(negative_wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\n",
    "    \"The top 100 most used words in the testing corpus\\nfor the negative users (without considering the \"\n",
    "    f\"{min_percentile}%\\nwords with least value of idf)\",\n",
    "    fontsize=70,\n",
    ")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "negative_mostcommon_small = FreqDist(result_negative_users_posts.split()).most_common(\n",
    "    25\n",
    ")\n",
    "\n",
    "x, y = zip(*negative_mostcommon_small)\n",
    "plt.figure(figsize=(50, 30))\n",
    "plt.margins(0.02)\n",
    "plt.bar(x, y)\n",
    "plt.xlabel(\"Words\", fontsize=50)\n",
    "plt.ylabel(\"Frequency of Words\", fontsize=50)\n",
    "plt.yticks(fontsize=40)\n",
    "plt.xticks(rotation=60, fontsize=40)\n",
    "plt.title(\n",
    "    \"Frecuency of the 25 most common words for the testing corpus\\nfor the negative users (without considering the \"\n",
    "    f\"{min_percentile}% words with least value of idf)\",\n",
    "    fontsize=60,\n",
    ")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
